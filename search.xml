<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ICASSP2019 TTS Summary(Oral)]]></title>
    <url>%2F2019%2F05%2F15%2FICASSP2019-TTS-Summary%2F</url>
    <content type="text"><![CDATA[LPCNET: Improving Neural Speech Synthesis through Linear Predictionpaper code 这篇文章主要是提出了一个基于线性预测的高质量声码器 主要贡献: 改进了wavernn的训练，更换了特征，使用了线性预测，同时减小模型，更容易训练 修正采样的过程，使得8bit的输出也能得到高音质的音频 训练中加入噪音，减小训练和生成之间的gap 稀疏化，向量化矩阵，对核心代码进行优化，代码开源 总结：这篇文章开源了代码，而且跑出来的效果真的很不错，力推！ Phonemic-level Duration Control Using Attention Alignment for Natural Speech Synthesispaper 这篇文章主要是为了解决tacotron在生成的时候不能局部的控制时长的问题以及训练时alignment比较难训好 主要贡献： 使用SPPAS得到phone序列的时长，把时长信息拼到encoder的convolution层的输出（包括起始点，终止点，时长）最后再过lstm，从而更容易的训练，合成的时候也能够控制时长 可以根据得到的时长信息生成alignment矩阵，和tacotron2预测的alignment之间算一个loss，相当于有一个hard align的teacher，更快的学到alignment的信息 不足： baseline实在太差 比较的时候应该是用的手动设置的时长，并没有单独的时长模型，结果不一定可靠 Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorizationpaper 这篇文章目的在于利用带noise的数据训练tacotron使得最后出来的是clean的音频，一般噪音没有label，同时和说话人极度相关，因此很难建模，本文利用了factorized latent variables分别建模说话人和其他声学信息（噪音等等）从而分离信息，最后可以用clean的信息来合成音频，基本架构可以参考之前的GMVAE 主要贡献： 提出了基于factorized latent variables的条件生成模型，分离说话人信息和其它信息 使用了说话人不相关的说话人数据增广，当然noise必须要有想要的label 对抗训练来更有效的分离信息 Representation Mixing for TTS Synthesispaper 这篇文章主要提出了兼容character和phoneme序列作为输出的思想，解决发音不对的问题。给两个sequence，一个字符序列，一个phoneme序列，然后一个mask序列，控制最后encoder的输入。同时提出了一个基于 L-BFGS 的合成方法。 主要贡献： 主要融合了character和phoneme两种序列 新的vocoder，虽然实时率不高，有效提升gl的质量 Robust and Fine-grained Prosody Control of End-to-end Speech Synthesispaper 这篇文章主要介绍了端到端语音合成中怎么在某个时刻变化韵律的问题。之前的很多文章比如gst，gmvae等等等都是global的embedding，不能做到对生成的声音的局部控制。文章中介绍了两种方法，分别在encoder端和decoder端加入prosody embedding。在encoder端，需要用prosody embedding和encoder outputs算一个attention来解决长度不匹配的问题。在decoder端，其实训练时reference audio和target audio sequece长度是一样的，但是作者使用了tacotron1的模型，一次decode r 帧，因此可以在reference encoder上加入stride或者做一个pooling保持长度一致。 主要贡献： 提出了局部控制韵律的方法，两种控制的方式，encoder和decoder端都可以 Neural Source-filter-based Waveform Model for Statistical Parametric Speech Synthesispaper 并行声码器，相比之前的很多的单独训练student模型，这里由f0产生激励信号，最后算多个stft loss。利用pytorch复现实验，demo. 主要贡献： 简化了之前训练student的难度，相比之前迭代的次数更少，训练的更快]]></content>
      <tags>
        <tag>paper</tag>
        <tag>icassp2019</tag>
        <tag>tts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LPCNET Improving Neural Speech Synthesis Through Linear Prediction]]></title>
    <url>%2F2019%2F05%2F14%2FLPCNET-IMPROVING-NEURAL-SPEECH-SYNTHESIS-THROUGH-LINEAR-PREDICTION%2F</url>
    <content type="text"><![CDATA[paper code 简介文章是基于18年2月份的WaveRNN来改进, 不同的是LPCNET是用来作为vocoder的，同时在相同的网络结构下能够得到更好的音质，每秒能进行30亿次的浮点数运算，能够支持在比较差的设备，手机等等上运行。实际上也是进行指令级的优化。 文章给了代码，从代码中其实能够学到很多:)。 基本框架基本模型如下图： 特征参数输入的音频为16k，提取的特征为20维，每一帧是10ms（160个采样点），特征包括18维的bark-scale cepstral coefficient和2维的音调参数（period, correlation）。网络训练的时候，每一个batch里面有20帧，由于帧级网络有3x1的卷积，因此接受域是5。 预加重和量化之前的wavenet是使用8 bit的u-law来量化信号点，使得最后输出为256的分类任务，这是由于音频的能量大都集中在基频，但是在高频仍然会有一些白噪声，尤其是16k的音频对高频有一定的倾斜。 为了避免这个问题，WaveRNN采用的高、低8位的方法使得最后的分布是16bit的。在这篇文章中，采用了一阶预加重滤波器过训练数据，滤波器如下： E(z) = 1 - \alpha z^{-1} \qquad \alpha=0.85合成结果可以通过逆滤波器得到最后的输出 D(z) = \frac{1}{1-\alpha z^{-1}}这种方法能够有效的减少噪声，并且使得8bit可以作为高音质音频的输出。 线性估测线性预测的值可以从之前的采样点计算 p_{t} = \sum_{k=1}^{M}a_{k}s_{t-k}$s_{t}$是采样点在t时刻的值，$a_k$是M阶线性预测系数的值，并且是可以根据18维的bark-scale cepstrum 计算出来的。 因此可以使用线性预测器来帮助神经网络进行训练，这里可以直接预测激励参数$e{t}=s{t}-p{t}$，而不是采样点的值$s{t}$，能够帮助减少u-law的噪音，因为激励参数的幅度相比原始信号点更低。 但是在输入并没有直接给$e{t-1}$，而是$e{t-1}, s_{t-1}, p_t$这样会更好的帮助训练。 输出层输出层上才用了双层的线性层来更容易的计算输出概率。 dual\text{\_}{fc}(x) = a_{1} \circ tanh(W_{1}x) + a_{2} \circ tanh(W_{2}x)$W_1$和$W_2$是线性层的权重，$a_1$和$a_2$是权重系数。论文中提到该方法有轻微的提升音频质量。 稀疏矩阵为了使得计算复杂度降低，对网络自回归部分比较大的权重$GRU_A$进行了稀疏化。如果对权重中元素逐个稀疏，会有效的阻止矢量化，因此才用了分块（block-sparse）的稀疏方法。 训练的时候从dense matrix开始，过程中将值比较低的块强制置为0，知道达到期望的稀疏度，实验发现16x1的块比较有效。 除了非零的块，还保留了稀疏矩阵对角线的值，因为这些值最有可能非零，同时对角线元素很容易进行矢量乘积。 Embedding和线性计算简化论文中对u-law之后的值并没有进行one-shot表示，而是过一个embedding层期望做到一个线性变化。 由于这个embedding得输出是直接给到gru层，因此可以对矩阵的计算做一定的简化，提前合并计算，减少计算复杂度。 主要就是将embedding的weight和non-recurrent的weight先做乘积得到新的embedding lookup table。 采样过程从概率中直接采样可能会给最后的输出带来噪音。 在18年icassp的FFTNet文章中，由于它的condition带有f0，因此会给非静音的概率值乘以一个常数$c=2$，有点类似基于temperature的采样。 在该文章中，同样有picth参数，因此没有做一个二分类的决策，而是才用下面的公式计算当前概率应该如何变化： c = 1 + max(0, 1.5g_p-0.5) \qquad 0 < g_p < 1$g_p$是pitch系数，在采样时，选取一个常数，保证任何在该常数以下的概率值都为0，最后重新分配概率，得到最后的概率分布。 P^{\prime}(e_t)=R(max[R([P(e_t)^c]) - T, 0])$R(\cdot)$表示重新分配概率，最后选取的T为0.002 训练加噪整个训练的输入如下： 因为在训练的时候，我们给的都是真实输入，但是在合成的时候是自回归式的合成，因此会造成训练和测试的不匹配，在训练的输入中加入噪音能够增强网络的鲁棒性。 由于有线性预测作为特征，因此加入的噪音的细节格外重要。 如果在训练的信号中加入噪音，预测干净的激励，这会使得合成的声音比较像分析合成的声码器。 但是像上图中加入噪音，能显著减少信号点上的误差，因为输入的线性预测同样用于输出的采样点的计算。 噪音是在ulaw之后加上的，分布为uniform [-3, 3]，实际加入噪音的方式可以参考代码，稍有变动。 实验结果复杂度整个网络除了前面并行的部分，大部分的计算都来自两个GRU和最后的线性层。因为模型相比WaveRNN要小，所以浮点数运算肯定会比较快，论文中为2.8GFLOPS和10GFLOPS，其他的诸如WaveNet，FFTNet，SampleRNN肯定会慢更多。 主观指标下图是和wavernn相比的指标： 样音]]></content>
      <tags>
        <tag>paper</tag>
        <tag>icassp2019</tag>
        <tag>tts</tag>
        <tag>vocoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sample-Efficient-Adaptive-Text-to-Speech]]></title>
    <url>%2F2019%2F02%2F23%2FSample-Efficient-Adaptive-Text-to-Speech%2F</url>
    <content type="text"><![CDATA[paper 文章主要是使用元学习的方法来做少量数据下的自适应tts。 训练： 输入是hts的文本特征和f0，经过上采样层作为wavenet的local condition。 使用多说话人的数据集来自适应说话人，因此会学习到一个说话人的embedding，这个embedding作为wavenet的global condition 三种自适应的策略： 在保持wavenet权重不变的情况下学习speaker embedding 用SGD fine-tuning整个模型 用训练好的speaker encoder来预测speaker embedding 基本框架如下： 给定speaker的身份为$s$，那么模型可以表达为： p(x|l,f0;e_{s}, w)=\prod_{t=1}^{T}p(x_t|x_{1:t-1}, l, f0;e_s,w)训练过程中的文本特征和f0是从成对的文本和音频从提取出来的，测试的时候都是用模型(参考heiga 2016年的文章)预测出来的。 自适应方法非参数化的自适应首先训练一个multi speaker的wavenet模型，框架如上，这样会得到两种参数： speaker embedding $e$ 共享的wavenet参数 $w$ 单独对speaker emebdding进行自适应针对一个新说话人的少量数据，随机初始化一个$e_{demo}$, 固定$w$, 输入该说话人的少量数据的文本特征和f0，来学习该说话人的embedding e_{demo}= \underset{e}{\textbf{argmax}}\sum_{i}\text{log}p(x_{demo}^{i}|l_{demo}^{i},{f0}_{demo}^{i};e,w) 对整个模型进行自适应相比上面，同时把$w$也加到自适应的参数里面 (e_{demo}, w_{finetuned})= \underset{e, w}{\textbf{argmax}}\sum_{i}\text{log}p(x_{demo}^{i}|l_{demo}^{i},{f0}_{demo}^{i};e,w)这两种方法都可以实现少量数据的自适应，但是训练过程不太一样，在第一种方法中，由于speaker embedding只是低维向量，不是那么容易过拟合，但是在第二种方法中，参数很多，自适应的时候容易对少量数据过拟合，因此需要留出10%的数据计算对数似然作为指标使得模型的训练在适当的时候能够停止，同时还需要采用1中训练好的embedding作为2中的初始化，同时模型的初始化能够极大的提升泛化能力。 参数化的自适应这里主要是用辅助的speaker encoder从给定的数据中预测speaker embedding，同时这个encoder和wavenet是从头一起训练的。 p(x|l,f0,x_{demo},l_{demo},{f0}_{demo};w)=\prod_{t=1}^Tp(x_t|x_{1:t-1},l,f0;e(x_{demo},l_{demo},{f0}_{demo}), w)去除说话人相关的信息给定的输出是文本特征和f0，但是f0是高度的说话人相关的信息，因此为了尽量让输入的特征说话人无关，因此对f0进行了normalize。 \hat{f0} := (f-E[f_{s}])/std(f_s)指标 自然度 相似度 另外论文中还给出了说话人识别的指标，就不贴出来了。]]></content>
      <tags>
        <tag>paper</tag>
        <tag>wavenet</tag>
        <tag>meta-learning</tag>
      </tags>
  </entry>
</search>
